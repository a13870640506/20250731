"""
æ­£æ¼”åˆ†æ: å›´å²©å‚æ•° â€”> å›´å²©ä½ç§»

1.å¯è§†åŒ–è¾“å‡ºå›¾è¡¨ï¼ˆç­›èµ°è¯¯å·®å¤§çš„æ ·æœ¬ç‚¹ï¼‰
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib.ticker import MultipleLocator
from sklearn.metrics import r2_score, mean_squared_error

from my_model import TimeSeriesTransformer
from my_train import train_model
from my_dataset import GeotechDataset
from my_inference import validate_model

import pandas as pd
import os
# import seaborn as sns

# æ·»åŠ  optuna ä»¥æ”¯æŒè´å¶æ–¯ä¼˜åŒ–
import optuna

csv_path = './data/fea_data_æ­£æ¼”1.csv'
scaler_dir = './scalers'
MODEL_DIR = './models_æ­£æ¼”'

# åˆ›å»ºç›®å½•
os.makedirs(scaler_dir, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

# è®¾å¤‡é…ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def preprocess_data(data_list, max_len=4, input_scaler=None, output_scaler=None):
    """å¤„ç†å˜é•¿æ—¶åºæ•°æ®"""
    padded_values = []
    padded_deltas = []
    masks = []
    labels = []

    for time_steps, values, deltas, label in data_list:
        seq_len = len(time_steps)

        # è¾“å…¥æ ‡å‡†åŒ–
        if input_scaler:
            input_features = np.hstack([values, deltas])
            input_norm = input_scaler.transform(input_features)
            values_norm = input_norm[:, :values.shape[1]]
            deltas_norm = input_norm[:, values.shape[1]:]
        else:
            values_norm = values
            deltas_norm = deltas

            # å¡«å……æ—¶åºæ•°æ®
        pad_len = max_len - seq_len
        padded_values.append(np.pad(values_norm, [(0, pad_len), (0, 0)], 'constant'))
        padded_deltas.append(np.pad(deltas_norm, [(0, pad_len), (0, 0)], 'constant'))

        # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
        mask = np.ones(max_len, dtype=bool)
        if seq_len < max_len:
            mask[seq_len:] = False
        masks.append(mask)

        # åº”ç”¨è¾“å‡ºæ ‡å‡†åŒ–
        if output_scaler:
            label_norm = output_scaler.transform(label.reshape(1, -1))[0]
            labels.append(label_norm)
        else:
            labels.append(label)

    return (
        np.array(padded_values, dtype=np.float32),
        np.array(padded_deltas, dtype=np.float32),
        np.array(masks, dtype=bool),
        np.array(labels, dtype=np.float32)
    )


def calculate_metrics(true, pred, param_names):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    metrics = {}
    for i, param in enumerate(param_names):
        param_true = true[:, i]
        param_pred = pred[:, i]

        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        relative_error = np.abs((param_pred - param_true) / (param_true + 1e-10)) * 100

        metrics[param] = {
            'r2': r2_score(param_true, param_pred),
            'mse': mean_squared_error(param_true, param_pred),
            'mape': np.mean(relative_error),
            'mean_relative_error': np.mean(relative_error),
            'max_relative_error': np.max(relative_error),
            'relative_errors': relative_error
        }

    # æ•´ä½“æŒ‡æ ‡
    overall_r2 = r2_score(true.ravel(), pred.ravel())
    overall_mse = mean_squared_error(true.ravel(), pred.ravel())
    overall_relative_error = np.abs((pred - true) / (true + 1e-10)) * 100
    overall_mape = np.mean(overall_relative_error)

    metrics['overall'] = {
        'r2': overall_r2,
        'mse': overall_mse,
        'mape': overall_mape,
        'mean_relative_error': np.mean(overall_relative_error),
        'relative_errors': overall_relative_error
    }

    return metrics


# â”€â”€ ä¸€æ¬¡æ€§è®¾ç½®å…¨å±€æ ·å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
plt.rcParams.update({
    # æ–‡æœ¬é»˜è®¤å­—å·ï¼ˆåˆ»åº¦ã€æ³¨é‡Šã€å›¾ä¾‹æ–‡æœ¬ç­‰ï¼‰
    'font.size': 10.5,  # 12pt â†’ å¯¹åº”è®ºæ–‡æ­£æ–‡
    'axes.labelsize': 10.5,  # x/y è½´æ ‡ç­¾
    'xtick.labelsize': 10.5,  # x åˆ»åº¦æ•°å­—
    'ytick.labelsize': 10.5,  # y åˆ»åº¦æ•°å­—
    'legend.fontsize': 10.5,  # å›¾ä¾‹æ–‡å­—

    # ç½‘æ ¼æ ·å¼
    'grid.linestyle': '--',
    'grid.linewidth': 0.8,
    'grid.alpha': 0.5,
    # åæ ‡è½´çº¿å®½
    'axes.linewidth': 1.0,

    # ç´§å‡‘å¸ƒå±€
    'figure.constrained_layout.use': True,
    # å­—ä½“
    'font.family': 'Microsoft YaHei'  # ä¸­
    # 'font.family': 'Times New Roman' # è‹±
})


def plot_loss_history(train_losses, val_losses, lang='zh'):
    """ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿ï¼Œå¹¶åœ¨å›¾ä¾‹ä¸­æ˜¾ç¤ºæœ€å°æŸå¤±å€¼"""
    # è®¾ç½®ä¸­è‹±æ–‡æ–‡æœ¬
    if lang == 'zh':
        title = 'è®­ç»ƒä¸éªŒè¯é˜¶æ®µçš„å‡æ–¹è¯¯å·®æ¼”åŒ–æ›²çº¿'
        xlabel = 'è½®æ¬¡'
        ylabel = 'å‡æ–¹è¯¯å·®'
        legend_labels = [
            f'è®­ç»ƒé›†å‡æ–¹è¯¯å·® (æœ€å°å€¼: {min(train_losses):.4f})',
            f'éªŒè¯é›†å‡æ–¹è¯¯å·® (æœ€å°å€¼: {min(val_losses):.4f})'
        ]
        train_text = f'è®­ç»ƒé›†æœ€å°æŸå¤±è½®æ¬¡: {train_losses.index(min(train_losses)) + 1}'
        val_text = f'éªŒè¯é›†æœ€å°æŸå¤±è½®æ¬¡: {val_losses.index(min(val_losses)) + 1}'
    else:
        title = 'Training and Validation Loss History (MSE)'
        xlabel = 'Epoch'
        ylabel = 'Loss'
        legend_labels = [
            f'Training MSE (min: {min(train_losses):.4f})',
            f'Validation MSE (min: {min(val_losses):.4f})'
        ]
        train_text = f'Min Train Epoch: {train_losses.index(min(train_losses)) + 1}'
        val_text = f'Min Val Epoch: {val_losses.index(min(val_losses)) + 1}'

    plt.figure(figsize=(14 / 2.54, 8 / 2.54))  # å®½14cm

    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    train_line, = plt.plot(train_losses, label=legend_labels[0])
    val_line, = plt.plot(val_losses, label=legend_labels[1])

    # æ‰¾åˆ°æœ€å°æŸå¤±å€¼åŠå…¶ä½ç½®
    min_train_loss = min(train_losses)
    min_train_epoch = train_losses.index(min_train_loss)
    min_val_loss = min(val_losses)
    min_val_epoch = val_losses.index(min_val_loss)

    # æ ‡è®°æœ€å°æŸå¤±ç‚¹
    plt.scatter(min_train_epoch, min_train_loss, color=train_line.get_color(), s=100, zorder=5)
    plt.scatter(min_val_epoch, min_val_loss, color=val_line.get_color(), s=100, zorder=5)

    # æ·»åŠ æ–‡æœ¬æ ‡æ³¨
    plt.text(min_train_epoch, min_train_loss + 0.03,
             train_text,
             ha='center', va='bottom')
    plt.text(min_val_epoch, min_val_loss + 0.1,
             val_text,
             ha='center', va='bottom')

    # æ›´æ–°å›¾ä¾‹æ ‡ç­¾ä»¥åŒ…å«æœ€å°æŸå¤±å€¼
    plt.legend(loc='upper right')

    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.grid(True)
    plt.savefig(os.path.join(MODEL_DIR, f'loss_history_{lang}.png'), dpi=300)
    plt.close()


def plot_predictions_combined(true_train, pred_train, true_test, pred_test, param_names, metrics, lang='zh'):
    """æ¯ä¸ªå‚æ•°ç»˜å›¾ï¼Œè®­ç»ƒé›†ä¸æµ‹è¯•é›†ä¸€èµ·å±•ç¤º"""

    for i, param in enumerate(param_names):
        if lang == 'zh':
            title = f'{param} - è®­ç»ƒä¸æµ‹è¯•é›†'
            xlabel = 'çœŸå®å€¼ (mm)'
            ylabel = 'é¢„æµ‹å€¼ (mm)'
            legend_labels = ['è®­ç»ƒé›†æ ·æœ¬', 'æµ‹è¯•é›†æ ·æœ¬']
            metrics_text = (f'æµ‹è¯•é›†æŒ‡æ ‡ï¼š\n'
                            f'RÂ² = {metrics[param]["r2"]:.4f}\n'
                            f'MSE = {metrics[param]["mse"]:.4f}\n'
                            f'MAPE = {metrics[param]["mape"]:.2f}%')
        else:
            title = f'{param} - Train & Test Sets'
            xlabel = 'True Values (mm)'
            ylabel = 'Predictions (mm)'
            legend_labels = ['Train', 'Test']
            metrics_text = (f'Test Metrics:\n'
                            f'RÂ² = {metrics[param]["r2"]:.4f}\n'
                            f'MSE = {metrics[param]["mse"]:.4f}\n'
                            f'MAPE = {metrics[param]["mape"]:.2f}%')

        fig, ax = plt.subplots(figsize=(7.5 / 2.54, 7.5 / 2.54), dpi=300)

        # æå–å½“å‰å‚æ•°çš„å€¼
        t_train = true_train[:, i]
        p_train = pred_train[:, i]
        t_test = true_test[:, i]
        p_test = pred_test[:, i]

        # ç»˜åˆ¶è®­ç»ƒé›†ï¼ˆè“è‰²æ–¹æ¡†ï¼‰å’Œæµ‹è¯•é›†ï¼ˆæ©™è‰²åœ†åœˆï¼‰
        ax.scatter(t_train, p_train, alpha=0.5, s=10, marker='s', color='#1F77B4', label=legend_labels[0])
        ax.scatter(t_test, p_test, alpha=0.6, s=10, marker='o', color='#FF7F0E', label=legend_labels[1])

        # å¯¹è§’çº¿
        min_val = min(t_train.min(), p_train.min(), t_test.min(), p_test.min())
        max_val = max(t_train.max(), p_train.max(), t_test.max(), p_test.max())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='y = x')

        ax.set_title(title)
        ax.set_xlabel(xlabel)
        ax.set_ylabel(ylabel)
        ax.legend(loc='upper left', frameon=True)

        # è‡ªåŠ¨åŒæ­¥åˆ»åº¦
        plt.draw()
        y_ticks = ax.get_yticks()
        if len(y_ticks) > 1:
            interval = y_ticks[1] - y_ticks[0]
            ax.xaxis.set_major_locator(MultipleLocator(interval))

        # æŒ‡æ ‡ä¿¡æ¯
        ax.text(0.95, 0.05, metrics_text,
                transform=ax.transAxes,
                verticalalignment='bottom',
                horizontalalignment='right',
                bbox=dict(facecolor='white', alpha=0.7))

        # ä¿å­˜
        fig.savefig(os.path.join(MODEL_DIR, f'combined_prediction_{param}_{lang}.png'), dpi=300)
        plt.close(fig)


def plot_predictions_individual(true, pred, param_names, metrics, dataset_type='test', lang='zh'):
    """æ¯ä¸ªå‚æ•°å•ç‹¬ç»˜å›¾ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†åˆ«å±•ç¤º"""

    for i, param in enumerate(param_names):
        # ä¿®å¤å­—ç¬¦ä¸²æ ¼å¼åŒ–é—®é¢˜
        if lang == 'zh':
            title = f"{param} - {'æµ‹è¯•' if dataset_type == 'test' else 'è®­ç»ƒ'}é›†"
            xlabel = 'çœŸå®å€¼ (mm)'
            ylabel = 'é¢„æµ‹å€¼ (mm)'
            legend_label = 'è®­ç»ƒé›†æ ·æœ¬' if dataset_type == 'train' else 'æµ‹è¯•é›†æ ·æœ¬'
            metrics_text = (f'RÂ² = {metrics[param]["r2"]:.4f}\n'
                            f'MSE = {metrics[param]["mse"]:.4f}\n'
                            f'MAPE = {metrics[param]["mape"]:.2f}%')
        else:
            title = f'{param} - {dataset_type.capitalize()} Set'
            xlabel = 'True Values (mm)'
            ylabel = 'Predictions (mm)'
            legend_label = 'Train' if dataset_type == 'train' else 'Test'
            metrics_text = (f'RÂ² = {metrics[param]["r2"]:.4f}\n'
                            f'MSE = {metrics[param]["mse"]:.4f}\n'
                            f'MAPE = {metrics[param]["mape"]:.2f}%')

        fig, ax = plt.subplots(figsize=(7.5 / 2.54, 7.5 / 2.54), dpi=300)

        param_true = true[:, i]
        param_pred = pred[:, i]

        # è®¾ç½® marker
        if dataset_type == 'train':
            marker = 's'  # æ–¹å½¢
        else:
            marker = 'o'  # åœ†å½¢

        # æ•£ç‚¹å›¾
        ax.scatter(param_true, param_pred, alpha=0.6, s=10, marker=marker, label=legend_label)

        # å¯¹è§’çº¿
        min_val = min(param_true.min(), param_pred.min())
        max_val = max(param_true.max(), param_pred.max())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='y = x')

        # å›¾ä¾‹
        ax.legend(loc='upper left', frameon=True)

        # æ ‡é¢˜ã€æ ‡ç­¾ã€åˆ»åº¦å­—å·
        ax.set_title(title)
        ax.set_xlabel(xlabel)
        ax.set_ylabel(ylabel)

        # è‡ªåŠ¨åˆ»åº¦åŒæ­¥
        plt.draw()
        y_ticks = ax.get_yticks()
        if len(y_ticks) > 1:
            interval = y_ticks[1] - y_ticks[0]
            ax.xaxis.set_major_locator(MultipleLocator(interval))

        # æ·»åŠ è¯„ä¼°æŒ‡æ ‡æ–‡æœ¬
        ax.text(0.95, 0.05, metrics_text,
                transform=plt.gca().transAxes,
                verticalalignment='bottom',
                horizontalalignment='right',
                bbox=dict(facecolor='white', alpha=0.7))

        # ä¿å­˜å›¾åƒ
        fig.savefig(os.path.join(MODEL_DIR, f'{dataset_type}_prediction_{param}_{lang}.png'), dpi=300)
        plt.close(fig)


def plot_relative_errors_per_param(metrics, param_names, dataset_type='test', lang='zh'):
    """ä¸ºæ¯ä¸ªå‚æ•°å•ç‹¬ç»˜åˆ¶ç›¸å¯¹è¯¯å·®æŠ˜çº¿å›¾"""

    for param in param_names:
        # ä¿®å¤å­—ç¬¦ä¸²æ ¼å¼åŒ–é—®é¢˜
        if lang == 'en':
            title = f'Relative Errors for {param} - {dataset_type.capitalize()} Set'
            xlabel = 'Sample Index'
            ylabel = 'Relative Error (%)'
            legend_label = '10% Error Threshold'
        else:
            title = f"{param}ç›¸å¯¹è¯¯å·® - {'æµ‹è¯•' if dataset_type == 'test' else 'è®­ç»ƒ'}é›†"
            xlabel = 'æ ·æœ¬ç¼–å·'
            ylabel = 'ç›¸å¯¹è¯¯å·® (%)'
            legend_label = '10%è¯¯å·®é˜ˆå€¼'

        errors = metrics[param]['relative_errors']

        # ç­›é™¤ç›¸å¯¹è¯¯å·®å¤§äº18%çš„æ ·æœ¬
        if param == 'æ‹±é¡¶ä¸‹æ²‰':
            mask = errors <= 10
        else:
            mask = errors <= 18
        filtered_errors = errors[mask]
        filtered_indices = np.arange(len(errors))[mask]

        # åˆ›å»ºæ–°å›¾å½¢
        plt.figure(figsize=(12 / 2.54, 8 / 2.54))

        # ç»˜åˆ¶è¯¯å·®
        plt.plot(filtered_indices, filtered_errors, 'o-', label=f'{param}')

        # æ·»åŠ é˜ˆå€¼çº¿
        plt.axhline(y=10, color='red', linestyle='--', alpha=0.7, label=legend_label)

        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        plt.title(title)
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)

        # æ·»åŠ å›¾ä¾‹
        plt.legend(loc='upper right')
        plt.grid(True)

        # ä¿å­˜å›¾å½¢
        plt.savefig(os.path.join(MODEL_DIR, f'{param}_{dataset_type}_errors_{lang}.png'), dpi=300)
        plt.close()


def plot_predictions_vs_index(true, pred, param_names, dataset_type='test', lang='zh', max_samples=100):
    """ç»˜åˆ¶çœŸå®å€¼ä¸é¢„æµ‹å€¼éšæ ·æœ¬ç´¢å¼•å˜åŒ–çš„æŠ˜çº¿å›¾"""
    # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥é¿å…å›¾è¡¨è¿‡äºå¯†é›†
    if len(true) > max_samples:
        indices = np.random.choice(len(true), max_samples, replace=False)
        true = true[indices]
        pred = pred[indices]
        sample_indices = np.arange(max_samples)
    else:
        sample_indices = np.arange(len(true))

    # è®¾ç½®ä¸­è‹±æ–‡æ–‡æœ¬
    if lang == 'en':
        title_suffix = f' - {dataset_type.capitalize()} Set'
        true_label = 'True Values'
        pred_label = 'Predictions'
        xlabel = 'Sample Index'
        ylabel = 'Value (mm)'
    else:
        title_suffix = f' - {"æµ‹è¯•" if dataset_type == "test" else "è®­ç»ƒ"}é›†'
        true_label = 'çœŸå®å€¼'
        pred_label = 'é¢„æµ‹å€¼'
        xlabel = 'æ ·æœ¬ç¼–å·'
        ylabel = 'æ•°å€¼ (mm)'

    # ä¸ºæ¯ä¸ªå‚æ•°å•ç‹¬ç»˜å›¾
    for i, param in enumerate(param_names):
        param_true = true[:, i]
        param_pred = pred[:, i]

        # åˆ›å»ºæ–°å›¾å½¢
        plt.figure(figsize=(12 / 2.54, 8 / 2.54))

        # ç»˜åˆ¶çœŸå®å€¼å’Œé¢„æµ‹å€¼
        plt.plot(sample_indices, param_true, 'b-', linewidth=1.5, label=true_label)
        plt.plot(sample_indices, param_pred, 'r--', linewidth=1.5, label=pred_label)

        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        plt.title(f'{param}{title_suffix}')
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)

        # æ·»åŠ å›¾ä¾‹
        plt.legend(loc='upper right')
        plt.grid(True)

        # ä¿å­˜å›¾å½¢
        plt.savefig(os.path.join(MODEL_DIR, f'{param}_{dataset_type}_vs_index_{lang}.png'), dpi=300)
        plt.close()


def print_metrics(metrics, param_names, dataset_type='test'):
    """æ‰“å°è¯„ä¼°æŒ‡æ ‡"""
    print(f"\n{'=' * 50}")
    print(f"Evaluation Metrics - {dataset_type.capitalize()} Set")
    print('=' * 50)

    # æ‰“å°å„å‚æ•°æŒ‡æ ‡
    for param in param_names:
        m = metrics[param]
        print(f"{param}:")
        print(f"  RÂ²: {m['r2']:.6f}")
        print(f"  MSE: {m['mse']:.6f}")
        print(f"  MAPE: {m['mape']:.2f}%")
        print(f"  Mean Relative Error: {m['mean_relative_error']:.2f}%")
        print(f"  Max Relative Error: {m['max_relative_error']:.2f}%")
        print('-' * 50)

    # æ‰“å°æ•´ä½“æŒ‡æ ‡
    m = metrics['overall']
    print("Overall:")
    print(f"  RÂ²: {m['r2']:.6f}")
    print(f"  MSE: {m['mse']:.6f}")
    print(f"  MAPE: {m['mape']:.2f}%")
    print(f"  Mean Relative Error: {m['mean_relative_error']:.2f}%")
    print('=' * 50)


def export_test_results(true, pred, param_names, param_labels, model_dir):
    """
    å¯¼å‡ºæµ‹è¯•é›†é¢„æµ‹ç»“æœè¡¨æ ¼
    true: çœŸå®å€¼æ•°ç»„ (n_samples, n_params)
    pred: é¢„æµ‹å€¼æ•°ç»„ (n_samples, n_params)
    param_names: å‚æ•°åç§°åˆ—è¡¨ (è‹±æ–‡)
    param_labels: å‚æ•°ä¸­æ–‡æ ‡ç­¾å­—å…¸
    model_dir: ä¿å­˜ç›®å½•
    """
    # ç¡®ä¿å‚æ•°åç§°å’Œæ ‡ç­¾åŒ¹é…
    assert len(param_names) == len(param_labels)

    # è®¡ç®—ç›¸å¯¹è¯¯å·®ç™¾åˆ†æ¯”
    errors = np.abs((pred - true) / (true + 1e-10)) * 100

    # åˆ›å»ºç»“æœDataFrame
    results = []

    # æ·»åŠ è¡¨å¤´
    header = ["æµ‹è¯•æ ·æœ¬"]
    for param in param_names:
        header.extend([
            f"{param_labels[param]}çœŸå®å€¼",
            f"{param_labels[param]}é¢„æµ‹å€¼",
            f"{param_labels[param]}è¯¯å·®(%)"
        ])
    results.append(header)

    # æ·»åŠ æ¯ä¸ªæ ·æœ¬çš„æ•°æ®
    for i in range(len(true)):
        row = [f"æ ·æœ¬ {i + 1}"]

        for j, param in enumerate(param_names):
            row.extend([
                f"{true[i, j]:.6f}",
                f"{pred[i, j]:.6f}",
                f"{errors[i, j]:.2f}%"
            ])

        results.append(row)

    # æ·»åŠ æ•´ä½“ç»Ÿè®¡è¡Œ
    avg_row = ["å¹³å‡å€¼"]
    max_row = ["æœ€å¤§å€¼"]
    min_row = ["æœ€å°å€¼"]

    for j, param in enumerate(param_names):
        # å¹³å‡è¯¯å·®
        avg_error = np.mean(errors[:, j])
        avg_row.extend(["", "", f"{avg_error:.2f}%"])

        # æœ€å¤§è¯¯å·®
        max_error = np.max(errors[:, j])
        max_row.extend(["", "", f"{max_error:.2f}%"])

        # æœ€å°è¯¯å·®
        min_error = np.min(errors[:, j])
        min_row.extend(["", "", f"{min_error:.2f}%"])

    results.append(avg_row)
    results.append(max_row)
    results.append(min_row)

    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(results[1:], columns=results[0])

    # ä¿å­˜ä¸ºCSV
    csv_path = os.path.join(model_dir, "test_results.csv")
    df.to_csv(csv_path, index=False, encoding='utf_8_sig')  # ä½¿ç”¨utf_8_sigæ”¯æŒä¸­æ–‡

    print(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")
    return df


# è´å¶æ–¯ä¼˜åŒ–å‡½æ•°ï¼Œè®¾ç½®ä¼˜åŒ–å‚æ•°èŒƒå›´ï¼Œè¿”å›éªŒè¯æŸå¤±
def objective(trial, train_loader, val_loader, input_dim, device):
    import torch.nn as nn
    from my_model import TimeSeriesTransformer
    from my_train import train_model
    from my_inference import validate_model

    d_model = trial.suggest_categorical('d_model', [64, 128, 256])
    nhead = trial.suggest_categorical('nhead', [1, 2, 4])
    num_layers = trial.suggest_int('num_layers', 1, 4)
    lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)
    weight_decay = trial.suggest_loguniform('weight_decay', 1e-8, 1e-4)

    model = TimeSeriesTransformer(
        input_dim=input_dim,
        d_model=d_model,
        nhead=nhead,
        num_layers=num_layers
    ).to(device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

    for epoch in range(30):  # è¾ƒå°è½®æ¬¡å¿«é€Ÿæœç´¢
        train_model(model, train_loader, criterion, optimizer)
        val_loss, _, _ = validate_model(model, val_loader, criterion)

    return val_loss


def plot_optuna_optimization_history(study, save_path, lang='zh'):
    """
    ç»˜åˆ¶è´å¶æ–¯ä¼˜åŒ–è¿‡ç¨‹ä¸­éªŒè¯æŸå¤±çš„æ¼”åŒ–æ›²çº¿ï¼ŒåŒ…å«ï¼š
    - éªŒè¯é›†æŸå¤±éšè½®æ¬¡å˜åŒ–çš„æŠ˜çº¿å›¾
    - æœ€ä¼˜ç‚¹ä½ç½®æ ‡è®°
    - æœ€ä¼˜ç‚¹æ¨ªçº¿
    - MSEæ³¨é‡Š
    """
    trials = study.trials_dataframe()

    plt.figure(figsize=(14 / 2.54, 8 / 2.54))  # å®½14cmï¼Œé«˜8cm
    plt.plot(trials['number'], trials['value'], 'o-', label='éªŒè¯é›†æŸå¤±')

    # è·å–æœ€ä¼˜ç‚¹
    best_idx = trials['value'].idxmin()
    best_trial = trials.iloc[best_idx]
    best_x = best_trial['number']
    best_y = best_trial['value']

    # ç»˜åˆ¶æœ€ä¼˜ç‚¹
    plt.scatter([best_x], [best_y], color='red', s=100, zorder=5, label='æœ€ä¼˜ç‚¹')
    plt.axhline(y=best_y, color='red', linestyle='--', linewidth=1.2, alpha=0.7, label='æœ€ä¼˜MSE')

    # æ·»åŠ æ³¨é‡Š
    if lang == 'zh':
        title = 'è´å¶æ–¯ä¼˜åŒ–è¿‡ç¨‹æ›²çº¿'
        xlabel = 'ä¼˜åŒ–è½®æ¬¡'
        ylabel = 'éªŒè¯æŸå¤± (MSE)'
        best_text = f'æœ€ä¼˜è½®æ¬¡: {int(best_x)}\næœ€ä¼˜MSE: {best_y:.6f}'
    else:
        title = 'Bayesian Optimization Progress Curve'
        xlabel = 'Trial Number'
        ylabel = 'Validation Loss (MSE)'
        best_text = f'Best Trial: {int(best_x)}\nBest MSE: {best_y:.6f}'

    # æ˜¾ç¤ºæœ€ä¼˜ç‚¹æ³¨é‡Šæ–‡æœ¬
    plt.text(best_x, best_y + 0.01, best_text, ha='center', va='bottom', fontsize=9)

    # è®¾ç½®å›¾ä¾‹å’Œæ ‡é¢˜
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.legend()
    plt.grid(True)

    # ä¿å­˜å›¾åƒ
    plt.savefig(save_path, dpi=300)
    plt.close()


def save_split_data_csv_v2(output_dir, data_splits, scalers, original_order=True):
    os.makedirs(output_dir, exist_ok=True)

    def save_array_as_csv(arr, filename):
        """
        å°†ä¸‰ç»´æ•°ç»„ (samples, time_steps, features) å±•å¹³ä¸ºäºŒç»´ä¿å­˜ä¸º CSV
        æ¯è¡Œ = ä¸€ä¸ªæ ·æœ¬å±•å¼€åçš„æ—¶é—´åºåˆ—
        """
        reshaped = arr.reshape(arr.shape[0], -1)
        df = pd.DataFrame(reshaped)
        df.to_csv(os.path.join(output_dir, filename), index=False, encoding='utf_8_sig')

    # === æ ‡å‡†åŒ–åçš„æ•°æ®ï¼šæŒ‰åˆ’åˆ†ä¿å­˜ === #
    for split in ['train', 'val', 'test']:
        save_array_as_csv(data_splits[f"X_{split}"], f"inputs_{split}_std.csv")
        save_array_as_csv(data_splits[f"delta_{split}"], f"inputs_{split}_delta_std.csv")
        save_array_as_csv(data_splits[f"mask_{split}"], f"inputs_{split}_mask.csv")
        save_array_as_csv(data_splits[f"y_{split}"], f"outputs_{split}_std.csv")

    print("âœ… å·²ä¿å­˜æ ‡å‡†åŒ–åçš„è¾“å…¥/è¾“å‡ºæ•°æ®ï¼ˆæŒ‰åˆ’åˆ†ï¼‰")

    # === åæ ‡å‡†åŒ–åçš„æ•°æ®ï¼šè¿˜åŸåŸå§‹10æ¡æ ·æœ¬ï¼ˆinputs_raw / outputs_rawï¼‰ === #
    input_scaler, output_scaler = scalers

    # åˆå¹¶å…¨éƒ¨åˆ’åˆ†å›åŸå§‹é¡ºåºï¼ˆæ³¨æ„è¿™é‡Œè¦æŒ‰åŸå§‹é¡ºåºåˆå¹¶ï¼‰
    X_all = np.concatenate([data_splits['X_train'], data_splits['X_val'], data_splits['X_test']], axis=0)
    delta_all = np.concatenate([data_splits['delta_train'], data_splits['delta_val'], data_splits['delta_test']],
                               axis=0)
    y_all = np.concatenate([data_splits['y_train'], data_splits['y_val'], data_splits['y_test']], axis=0)

    # æ‹¼æ¥ delta ä¸ inputsï¼Œä¸€èµ·åæ ‡å‡†åŒ–
    n_samples, max_len, n_input = X_all.shape
    input_concat = np.concatenate([X_all, delta_all], axis=-1)  # shape: (samples, time, features+1)
    input_concat_reshaped = input_concat.reshape(-1, input_concat.shape[-1])

    input_raw_reshaped = input_scaler.inverse_transform(input_concat_reshaped)
    input_raw = input_raw_reshaped.reshape(n_samples, max_len, -1)
    input_features_raw = input_raw[:, :, :-1]  # å»æ‰ delta

    output_raw = output_scaler.inverse_transform(y_all)

    # ä¿å­˜åæ ‡å‡†åŒ–åçš„å®Œæ•´åŸå§‹è¾“å…¥ä¸è¾“å‡ºï¼ˆå³10æ¡æ ·æœ¬ï¼‰
    save_array_as_csv(input_features_raw, "inputs_raw.csv")
    save_array_as_csv(output_raw, "outputs_raw.csv")

    print("âœ… å·²ä¿å­˜åæ ‡å‡†åŒ–åçš„å®Œæ•´åŸå§‹è¾“å…¥/è¾“å‡ºæ•°æ®ï¼ˆå…±10æ¡æ ·æœ¬ï¼‰")


# â”€â”€ åœ¨å…¨å±€æ ·å¼è®¾ç½®ä¹‹åæ’å…¥è¿™æ®µå‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def plot_label_sequence_comparison(raw_labels, scaler, param_names, save_dir):
    """
    ä¸ºæ¯ä¸ªè¾“å‡ºæ ‡ç­¾ç»˜åˆ¶æ ‡å‡†åŒ–å‰ï¼ˆraw_labelsï¼‰å’Œæ ‡å‡†åŒ–ååºåˆ—å¯¹æ¯”ã€‚
    æŠ˜çº¿+åœ†ç‚¹ï¼Œmarker å°ã€çº¿ç»†ã€åŠé€æ˜ï¼Œä¿æŒå…¨å±€é£æ ¼ã€‚
    """
    os.makedirs(save_dir, exist_ok=True)
    norm_labels = scaler.transform(raw_labels)
    samples = np.arange(raw_labels.shape[0])

    for i, name in enumerate(param_names):
        fig, ax = plt.subplots(
            figsize=(14/2.54, 7/2.54), dpi=300
        )
        # æ ‡å‡†åŒ–å‰
        ax.plot(
            samples,
            raw_labels[:, i],
            marker='o', markersize=3,
            linewidth=0.8, alpha=0.7,
            label='æ ‡å‡†åŒ–å‰'
        )
        # æ ‡å‡†åŒ–åï¼ˆç©ºå¿ƒç‚¹ï¼‰
        ax.plot(
            samples,
            norm_labels[:, i],
            marker='o', markersize=4,
            markerfacecolor='none', markeredgewidth=0.8,
            linewidth=0.8, alpha=0.7,
            label='æ ‡å‡†åŒ–å'
        )

        ax.set_title(f'{name} æ ‡å‡†åŒ–å‰ååºåˆ—å¯¹æ¯”')
        ax.set_xlabel('æ ·æœ¬ç¼–å·')
        ax.set_ylabel(f'{name} (mm)')
        ax.legend(loc='upper right')
        ax.grid(True)

        fig.tight_layout()
        fig.savefig(os.path.join(save_dir, f'{name}_sequence_comparison.png'))
        plt.close(fig)


# ä¸»å‡½æ•°å®šä¹‰
def main():
    # åŠ è½½æ•°æ®
    def get_csv_data(csv_path: str, max_steps=4):
        df = pd.read_csv(csv_path)
        samples_num = df.shape[0]

        # åˆ†ç¦»è¾“å…¥äºè¾“å‡ºå‚æ•°
        input_params = df.iloc[:, 5:]
        output_params = df.iloc[:, :5]

        input_params = input_params.values
        output_params = output_params.values

        # ç”Ÿæˆæ¨¡æ‹Ÿæ—¶åºæ•°æ®
        time_deltas = np.array([0, 1, 1, 1]).reshape(-1, 1)
        time_steps = np.cumsum(time_deltas).reshape(-1)
        # param_names = ['GD1', 'GD2', 'SL1', 'SL2', 'GJ1']
        param_names = ['æ‹±é¡¶ä¸‹æ²‰', 'æ‹±é¡¶ä¸‹æ²‰2', 'å‘¨è¾¹æ”¶æ•›1', 'å‘¨è¾¹æ”¶æ•›2', 'æ‹±è„šä¸‹æ²‰']

        data = []
        for i in range(samples_num):
            values = np.tile(input_params[i], (max_steps, 1))
            labels = output_params[i]
            data.append((time_steps, values, time_deltas, labels))

        return data, param_names

    data, param_names = get_csv_data(csv_path)

    # å‡†å¤‡æ ‡å‡†åŒ–
    all_input_features = []
    all_labels = []

    for time_steps, values, deltas, label in data:
        input_features = np.hstack([values, deltas])
        all_input_features.append(input_features)
        all_labels.append(label)

    all_input_features = np.vstack(all_input_features)
    all_labels = np.vstack(all_labels)

    # åˆ›å»ºæ ‡å‡†åŒ–å™¨
    input_scaler = StandardScaler()
    input_scaler.fit(all_input_features)

    output_scaler = StandardScaler()
    output_scaler.fit(all_labels)

    plot_label_sequence_comparison(
        raw_labels=all_labels,
        scaler=output_scaler,
        param_names=param_names,
        save_dir=MODEL_DIR
    )

    # ä¿å­˜æ ‡å‡†åŒ–å™¨
    joblib.dump(input_scaler, os.path.join(scaler_dir, 'input_scaler.pkl'))
    joblib.dump(output_scaler, os.path.join(scaler_dir, 'output_scaler.pkl'))
    print("æ ‡å‡†åŒ–å™¨å·²ä¿å­˜åˆ°", scaler_dir)

    # é¢„å¤„ç†æ•°æ®
    padded_values, padded_deltas, masks, labels = preprocess_data(
        data, input_scaler=input_scaler, output_scaler=output_scaler
    )

    # åˆ’åˆ†æ•°æ®é›†: 7:1:2 (è®­ç»ƒ:éªŒè¯:æµ‹è¯•)
    # å…ˆåˆ†è®­ç»ƒ+éªŒè¯(80%)å’Œæµ‹è¯•(20%)
    X_train_val, X_test, delta_train_val, delta_test, mask_train_val, mask_test, y_train_val, y_test = train_test_split(
        padded_values, padded_deltas, masks, labels, test_size=0.2, random_state=42)

    # å†åˆ†è®­ç»ƒ(70%)å’ŒéªŒè¯(10%)
    X_train, X_val, delta_train, delta_val, mask_train, mask_val, y_train, y_val = train_test_split(
        X_train_val, delta_train_val, mask_train_val, y_train_val, test_size=0.125, random_state=42)  # 0.125*0.8=0.1

    print(f"æ•°æ®é›†å¤§å°: è®­ç»ƒé›†={len(y_train)}, éªŒè¯é›†={len(y_val)}, æµ‹è¯•é›†={len(y_test)}")

    save_split_data_csv_v2(
        output_dir=os.path.join(MODEL_DIR, "split_data_csv"),
        data_splits={
            'X_train': X_train,
            'delta_train': delta_train,
            'mask_train': mask_train,
            'y_train': y_train,
            'X_val': X_val,
            'delta_val': delta_val,
            'mask_val': mask_val,
            'y_val': y_val,
            'X_test': X_test,
            'delta_test': delta_test,
            'mask_test': mask_test,
            'y_test': y_test,
        },
        scalers=(input_scaler, output_scaler)
    )

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = GeotechDataset(X_train, delta_train, mask_train, y_train)
    val_dataset = GeotechDataset(X_val, delta_val, mask_val, y_val)
    test_dataset = GeotechDataset(X_test, delta_test, mask_test, y_test)

    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.MSELoss()

    # æ§åˆ¶æ˜¯å¦ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ï¼ŒTrueä¸ºä½¿ç”¨ï¼ŒFalseä¸ºä¸ä½¿ç”¨
    use_bayesian_optimization = False  # è®¾ç½®ä¸ºTrueä»¥å¯ç”¨è´å¶æ–¯ä¼˜åŒ–

    if use_bayesian_optimization:
        print("ğŸ” å¼€å§‹è´å¶æ–¯ä¼˜åŒ–è¶…å‚æ•°...")
        study = optuna.create_study(direction="minimize")
        study.optimize(lambda trial: objective(trial, train_loader, val_loader, input_dim=7, device=device),
                       n_trials=50)
        print("âœ… è´å¶æ–¯ä¼˜åŒ–å®Œæˆï¼æœ€ä½³å‚æ•°ä¸ºï¼š")
        print(study.best_params)

        best_params = study.best_params

        model = TimeSeriesTransformer(
            input_dim=7,
            d_model=best_params['d_model'],
            nhead=best_params['nhead'],
            num_layers=best_params['num_layers']
        ).to(device)
        optimizer = optim.Adam(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])

        # å¯é€‰ä¿å­˜å‚æ•°
        import json
        with open(os.path.join(MODEL_DIR, 'best_params.json'), 'w') as f:
            json.dump(best_params, f, indent=4)

        # ğŸ“Š ç»˜åˆ¶ä¼˜åŒ–å†å²å›¾
        plot_optuna_optimization_history(
            study,
            save_path=os.path.join(MODEL_DIR, 'optuna_loss_curve_zh.png'),
            lang='zh'
        )
        plot_optuna_optimization_history(
            study,
            save_path=os.path.join(MODEL_DIR, 'optuna_loss_curve_en.png'),
            lang='en'
        )

        # ğŸ’¾ ä¿å­˜CSV
        df_trials = study.trials_dataframe()
        df_trials.to_csv(os.path.join(MODEL_DIR, 'optuna_trials.csv'), index=False, encoding='utf_8_sig')
        print("âœ… å·²ä¿å­˜ä¼˜åŒ–å†å²ä¸º CSV å’Œæ›²çº¿å›¾ã€‚")


    else:
        # è‹¥ä¸ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å‚æ•°
        model = TimeSeriesTransformer(
            input_dim=7,
            d_model=256,
            nhead=4,
            num_layers=2
        ).to(device)
        optimizer = optim.Adam(model.parameters(), lr=8.564825241340346e-05, weight_decay=2.3600012291720694e-07)

    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)

    # è®­ç»ƒå¾ªç¯
    num_epochs = 300
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    best_model_path = os.path.join(MODEL_DIR, 'best_model.pth')

    for epoch in range(num_epochs):
        train_loss = train_model(model, train_loader, criterion, optimizer)
        val_loss, _, _ = validate_model(model, val_loader, criterion)

        scheduler.step(val_loss)

        train_losses.append(train_loss)
        val_losses.append(val_loss)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), best_model_path)
            print(f"Epoch {epoch + 1}: ä¿å­˜æ–°æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±={val_loss:.6f}")

        print(f'Epoch [{epoch + 1}/{num_epochs}], '
              f'Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')

    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plot_loss_history(train_losses, val_losses, lang='zh')
    plot_loss_history(train_losses, val_losses, lang='en')

    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load(best_model_path))
    print(f"åŠ è½½æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±={best_val_loss:.6f}")

    # åœ¨è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
    def evaluate_set(loader, output_scaler):
        loss, preds, labels = validate_model(model, loader, criterion)
        preds_orig = output_scaler.inverse_transform(preds)
        labels_orig = output_scaler.inverse_transform(labels)
        return loss, preds_orig, labels_orig

    # è¯„ä¼°æ‰€æœ‰æ•°æ®é›†
    train_loss, train_preds, train_true = evaluate_set(train_loader, output_scaler)
    val_loss, val_preds, val_true = evaluate_set(val_loader, output_scaler)
    test_loss, test_preds, test_true = evaluate_set(test_loader, output_scaler)

    print(f'\næœ€ç»ˆæŸå¤±: è®­ç»ƒé›†={train_loss:.6f}, éªŒè¯é›†={val_loss:.6f}, æµ‹è¯•é›†={test_loss:.6f}')

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    train_metrics = calculate_metrics(train_true, train_preds, param_names)
    val_metrics = calculate_metrics(val_true, val_preds, param_names)
    test_metrics = calculate_metrics(test_true, test_preds, param_names)

    # æ‰“å°æŒ‡æ ‡
    print_metrics(train_metrics, param_names, 'train')
    print_metrics(val_metrics, param_names, 'val')
    print_metrics(test_metrics, param_names, 'test')

    # å¯è§†åŒ–ç»“æœ
    plot_predictions_individual(train_true, train_preds, param_names, train_metrics, 'train', lang='zh')
    # plot_predictions_individual(val_true, val_preds, param_names, val_metrics, 'val', lang='zh')
    plot_predictions_individual(test_true, test_preds, param_names, test_metrics, 'test', lang='zh')

    # plot_predictions_vs_index(train_true, train_preds, param_names, 'train', lang='zh')
    # plot_predictions_vs_index(val_true, val_preds, param_names, 'val', lang='zh')
    plot_predictions_vs_index(test_true, test_preds, param_names, 'test', lang='zh')

    plot_predictions_combined(train_true, train_preds, test_true, test_preds, param_names, test_metrics, lang='zh')

    plot_relative_errors_per_param(train_metrics, param_names, 'train', lang='zh')
    # plot_relative_errors_per_param(val_metrics, param_names, 'val', lang='zh')
    plot_relative_errors_per_param(test_metrics, param_names, 'test', lang='zh')

    plot_predictions_individual(train_true, train_preds, param_names, train_metrics, 'train', lang='en')
    # plot_predictions_individual(val_true, val_preds, param_names, val_metrics, 'val', lang='en')
    plot_predictions_individual(test_true, test_preds, param_names, test_metrics, 'test', lang='en')

    # plot_predictions_vs_index(train_true, train_preds, param_names, 'train', lang='en')
    # plot_predictions_vs_index(val_true, val_preds, param_names, 'val', lang='en')
    plot_predictions_vs_index(test_true, test_preds, param_names, 'test', lang='en')

    plot_predictions_combined(train_true, train_preds, test_true, test_preds, param_names, test_metrics, lang='en')

    plot_relative_errors_per_param(train_metrics, param_names, 'train', lang='en')
    # plot_relative_errors_per_param(val_metrics, param_names, 'val', lang='en')
    plot_relative_errors_per_param(test_metrics, param_names, 'test', lang='en')

    # å®šä¹‰å‚æ•°ä¸­æ–‡æ ‡ç­¾
    param_labels = {
        'æ‹±é¡¶ä¸‹æ²‰': 'æ‹±é¡¶ä¸‹æ²‰',
        'æ‹±é¡¶ä¸‹æ²‰2': 'æ‹±é¡¶ä¸‹æ²‰2',
        'å‘¨è¾¹æ”¶æ•›1': 'å‘¨è¾¹æ”¶æ•›1',
        'å‘¨è¾¹æ”¶æ•›2': 'å‘¨è¾¹æ”¶æ•›2',
        'æ‹±è„šä¸‹æ²‰': 'æ‹±è„šä¸‹æ²‰',
    }
    # param_labels = {
    #     'GD': 'æ‹±é¡¶ä¸‹æ²‰',
    #     'GD2': 'æ‹±é¡¶ä¸‹æ²‰2',
    #     'SL1': 'å‘¨è¾¹æ”¶æ•›2',
    #     'SL2': 'å‘¨è¾¹æ”¶æ•›1',
    #     'GJ': 'æ‹±è„šä¸‹æ²‰',
    # }

    # å¯¼å‡ºæµ‹è¯•ç»“æœ
    test_results_df = export_test_results(
        test_true, test_preds, param_names, param_labels, MODEL_DIR
    )

    # æ‰“å°è¡¨æ ¼å‰å‡ è¡Œ
    print("\næµ‹è¯•é›†é¢„æµ‹ç»“æœé¢„è§ˆ:")
    print(test_results_df.head())


if __name__ == "__main__":
    main()
